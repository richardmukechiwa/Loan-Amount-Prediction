{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml\\\\Loan-Amount-Prediction'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from constants and utils\n",
    "from credit_risk.constants import *\n",
    "from credit_risk.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating a Configuration class\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath   = CONFIG_FILE_PATH,\n",
    "        params_filepath   = PARAMS_FILE_PATH,\n",
    "        schema_filepath   = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)                       \n",
    "        self.schema = read_yaml(schema_filepath)  \n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self)->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir  = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "        )\n",
    "        \n",
    "        return data_transformation_config\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from credit_risk import logger\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"C:/Users/RICH-FILES/Desktop/ml/Loan-Amount-Prediction\"\n",
    "\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #transform categorical data and standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline   \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataTransformationConfig:\n",
    "    def __init__(self, data_path, model_path):\n",
    "        self.data_path = data_path\n",
    "        self.model_path = model_path\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def data_cleaning(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        \n",
    "        # Remove columns which are not necessary for the analysis\n",
    "        data.drop(columns=[\"Id\", \"Status\", \"Default\"], inplace=True)\n",
    "        \n",
    "        # Drop null values\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        logger.info(\"Null values dropped\")\n",
    "        \n",
    "        # Remove outliers\n",
    "        data = data[(data['Age'] < 80) & (data['Emp_length'] < 10) & (data['Income'] < 948000)]\n",
    "        \n",
    "        logger.info(\"Data cleaning complete\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def exploratory_data_analysis(self, data):\n",
    "        # Check descriptive statistics\n",
    "        print(data.describe())\n",
    "        \n",
    "        # Check non-numeric columns\n",
    "        print(data.describe(include='object'))\n",
    "        \n",
    "        # Check the target variable\n",
    "        data['Amount'].hist()\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Amount')    \n",
    "        plt.title('Loan Amount Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"The distribution is right-skewed, meaning most loan amounts fall in the lower range (below 10,000), while fewer loans exist at higher amounts\")\n",
    "        \n",
    "        # Calculate Amount distribution by Age   \n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.scatterplot(x='Age', y='Amount', data=data) \n",
    "        plt.xlabel('Age')\n",
    "        plt.ylabel('Amount')            \n",
    "        plt.title('Loan Amount by Age')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate Amount distribution by Income\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.scatterplot(x='Income', y='Amount', data=data)    \n",
    "        plt.xlabel('Income')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Loan Amount by Income')\n",
    "        plt.show()\n",
    "        \n",
    "        # Loan purpose count\n",
    "        plt.figure(figsize=(12,6))\n",
    "        data[\"Intent\"].value_counts().plot(kind='bar')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Intent')\n",
    "        plt.title('Loan Intent Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        # Check multicollinearity and correlation\n",
    "        plt.figure(figsize=(12,6))  \n",
    "        corr = data.select_dtypes(include=['int64', 'float64']).drop('Amount', axis=1).corr()    \n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Drop column Id and Cred_length (if they exist)\n",
    "        columns_to_drop = [\"Cred_length\"]\n",
    "        data.drop(columns=[col for col in columns_to_drop if col in data.columns], inplace=True)\n",
    "        \n",
    "        pd.options.mode.copy_on_write = True\n",
    "        print(data.head())\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def feat_engineering(self, data):\n",
    "        \n",
    "        # Define categorical and numerical features\n",
    "        cat_features = [\"Home\", \"Intent\"]\n",
    "        num_features = [\"Age\", \"Income\", \"Emp_length\", \"Amount\", \"Rate\", \"Percent_income\"]\n",
    "        \n",
    "        # Implement the column transformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features),\n",
    "                (\"num\", StandardScaler(), num_features)\n",
    "            ]\n",
    "        )   \n",
    "        \n",
    "        pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    " \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(data)\n",
    "        \n",
    "        # Save the pipeline\n",
    "        joblib.dump(pipeline, self.config.model_path)\n",
    "        \n",
    "        # Transform the data\n",
    "        transformed_data = pipeline.transform(data)\n",
    "        \n",
    "        # Create DataFrame from the transformed data\n",
    "        transformed_df = pd.DataFrame(transformed_data, columns=num_features + preprocessor.named_transformers_[\"cat\"].get_feature_names_out().tolist())\n",
    "        transformed_df.to_csv(\"artifacts/data_ingestion/credit_risk.csv\", index=False)     \n",
    "        \n",
    "        return transformed_df\n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataTransformationConfig:\n",
    "    def __init__(self, data_path, model_path, root_dir):\n",
    "        self.data_path = data_path\n",
    "        self.model_path = model_path\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def data_cleaning(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        \n",
    "        # Remove columns which are not necessary for the analysis\n",
    "        data.drop(columns=[\"Id\", \"Status\", \"Default\"], inplace=True)\n",
    "        \n",
    "        # Drop null values\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        logger.info(\"Null values dropped\")\n",
    "        \n",
    "        # Remove outliers\n",
    "        data = data[(data['Age'] < 80) & (data['Emp_length'] < 10) & (data['Income'] < 948000)]\n",
    "        \n",
    "        logger.info(\"Data cleaning complete\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def exploratory_data_analysis(data):\n",
    "        # Check descriptive statistics\n",
    "        print(data.describe())\n",
    "        \n",
    "        # Check non-numeric columns\n",
    "        print(data.describe(include='object'))\n",
    "        \n",
    "        # Check the target variable\n",
    "        data['Amount'].hist()\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Amount')    \n",
    "        plt.title('Loan Amount Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"The distribution is right-skewed, meaning most loan amounts fall in the lower range (below 10,000), while fewer loans exist at higher amounts.\")\n",
    "        \n",
    "        # Calculate Amount distribution by Age   \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.scatterplot(x='Age', y='Amount', data=data) \n",
    "        plt.xlabel('Age')\n",
    "        plt.ylabel('Amount')            \n",
    "        plt.title('Loan Amount by Age')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate Amount distribution by Income\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.scatterplot(x='Income', y='Amount', data=data)    \n",
    "        plt.xlabel('Income')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Loan Amount by Income')\n",
    "        plt.show()\n",
    "        \n",
    "        # Loan purpose count\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        data[\"Intent\"].value_counts().plot(kind='bar')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Intent')\n",
    "        plt.title('Loan Intent Distribution')\n",
    "        plt.show()\n",
    "        \n",
    "        # Check multicollinearity and correlation\n",
    "        plt.figure(figsize=(12, 6))  \n",
    "        corr = data.select_dtypes(include=['int64', 'float64']).drop('Amount', axis=1).corr()    \n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Drop column Cred_length (if it exists)\n",
    "        columns_to_drop = [\"Cred_length\"]\n",
    "        data.drop(columns=[col for col in columns_to_drop if col in data.columns], inplace=True)\n",
    "        \n",
    "        pd.options.mode.copy_on_write = True\n",
    "        print(data.head())\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def feat_engineering(self, data):\n",
    "        \n",
    "        # Define categorical and numerical features\n",
    "        cat_features = [\"Home\", \"Intent\"]\n",
    "        num_features = [\"Age\", \"Income\", \"Emp_length\", \"Amount\", \"Rate\", \"Percent_income\"]\n",
    "        \n",
    "        # Implement the column transformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_features),\n",
    "                (\"num\", StandardScaler(), num_features)\n",
    "            ]\n",
    "        )   \n",
    "        \n",
    "        pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n",
    " \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(data)\n",
    "        \n",
    "        # Save the pipeline\n",
    "        joblib.dump(pipeline, self.config.model_path)\n",
    "        \n",
    "        # Transform the data\n",
    "        transformed_data = pipeline.transform(data)\n",
    "        \n",
    "        # Create DataFrame from the transformed data\n",
    "        transformed_df = pd.DataFrame(transformed_data, columns=num_features + preprocessor.named_transformers_[\"cat\"].get_feature_names_out().tolist())\n",
    "        transformed_csv_path = os.path.join(self.config.root_dir, \"credit_risk.csv\")\n",
    "        transformed_df.to_csv(transformed_csv_path, index=False)     \n",
    "        \n",
    "        return transformed_csv_path\n",
    "    \n",
    "    def train_test_splitting(self, data_path):\n",
    "        data = pd.read_csv(data_path)\n",
    "        \n",
    "        # Split the data into train and test\n",
    "        train, test = train_test_split(data, test_size=0.2, random_state=42)  \n",
    "        \n",
    "        train.to_csv(os.path.join(self.config.root_dir, 'train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, 'test.csv'), index=False) \n",
    "        \n",
    "        # Save the train and test data to the root directory\n",
    "        logger.info(\"Data split into train and test data\")  \n",
    "        logger.info(f\"Train data shape: {train.shape}\")         \n",
    "        logger.info(f\"Test data shape: {test.shape}\")  \n",
    "        \n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-19 22:29:49,572: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-19 22:29:49,576: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-19 22:29:49,582: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-19 22:29:49,584: INFO: common: created directory at: artifacts]\n",
      "[2025-03-19 22:29:49,584: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataTransformationConfig.__init__() missing 1 required positional argument: 'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mtrain_test_splitting(transformed_data)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[88], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     config \u001b[38;5;241m=\u001b[39m ConfigurationManager()\n\u001b[1;32m----> 3\u001b[0m     data_transformation_config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_transformation_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m     data_transformation \u001b[38;5;241m=\u001b[39m DataTransformation(config \u001b[38;5;241m=\u001b[39m data_transformation_config)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Perform data transformation steps\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[82], line 20\u001b[0m, in \u001b[0;36mConfigurationManager.get_data_transformation_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_transformation\n\u001b[0;32m     18\u001b[0m create_directories([config\u001b[38;5;241m.\u001b[39mroot_dir])\n\u001b[1;32m---> 20\u001b[0m data_transformation_config \u001b[38;5;241m=\u001b[39m \u001b[43mDataTransformationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_transformation_config\n",
      "\u001b[1;31mTypeError\u001b[0m: DataTransformationConfig.__init__() missing 1 required positional argument: 'model_path'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config() \n",
    "    data_transformation = DataTransformation(config = data_transformation_config)\n",
    "    # Perform data transformation steps\n",
    "    cleaned_data = data_transformation.data_cleaning()\n",
    "    analyzed_data = data_transformation.exploratory_data_analysis(cleaned_data)\n",
    "    transformed_data = data_transformation.feat_engineering(analyzed_data)\n",
    "    train_data, test_data = data_transformation.train_test_splitting(transformed_data)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
