{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml\\\\Loan-Amount-Prediction'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RICH-FILES\\\\Desktop\\\\ml'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from constants and utils\n",
    "from credit_risk.constants import *\n",
    "from credit_risk.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating a Configuration class\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath   = CONFIG_FILE_PATH,\n",
    "        params_filepath   = PARAMS_FILE_PATH,\n",
    "        schema_filepath   = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)                       \n",
    "        self.schema = read_yaml(schema_filepath)  \n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self)->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir  = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "        )\n",
    "        \n",
    "        return data_transformation_config\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from credit_risk import logger\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"C:/Users/RICH-FILES/Desktop/ml/Loan-Amount-Prediction\"\n",
    "\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #transform categorical data and standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline   \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "     # add EDA to the data \n",
    "    def data_cleaning(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        \n",
    "        #remove  columns which are not necessary for the analysis\n",
    "        data.drop(columns = [\"Id\",\"Status\", \"Default\"], inplace=True)\n",
    "        \n",
    "        \n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        print(\"...............................................................\")\n",
    "        #drop null values\n",
    "        print(data.isnull().sum())\n",
    "        \n",
    "        print(\"...............................................................\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        logger.info(f\"Null values dropped\")\n",
    "        \n",
    "        print(\"................................................................\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #remove outliers\n",
    "        data = data[(data['Age'] < 80) & (data['Emp_length'] < 10) & (data['Income'] < 948000)]\n",
    "        \n",
    "        \n",
    "        print(\"data.head()\")\n",
    "        \n",
    "        print(data.head())\n",
    "        \n",
    "        print(\"...................................................................\")\n",
    "        \n",
    "        data1  = data\n",
    "        \n",
    "        return data1\n",
    "\n",
    "        logger.info(f\"Data cleaning complete\")\n",
    "        \n",
    "    \n",
    "      \n",
    "        \n",
    "    def exploratory_data_analysis(self):\n",
    "        data1 = pd.read_csv(self.config.data_path)\n",
    "        \n",
    "        data1 = data1[(data1['Age'] < 80) & (data1['Emp_length'] < 10) & (data1['Income'] < 948000)]\n",
    "        \n",
    "        #check descriptive statistics\n",
    "        \n",
    "        print(data1.describe())\n",
    "        \n",
    "        #check non numeric columns\n",
    "        print(data1.describe(include='object'))\n",
    "        \n",
    "        #check the target variable\n",
    "        data1['Amount'].hist()\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Amount')    \n",
    "        plt.title('Loan Amount Distribution')\n",
    "        \n",
    "        \n",
    "        print(\"The distribution is right-skewed, meaning most loan amounts fall in the lower range (below 10,000), while fewer loans exist at higher amounts\");\n",
    "                \n",
    "        #calculate Amount distribution by Age   \n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.scatterplot(x='Age', y='Amount', data=data1) \n",
    "        plt.xlabel('Age')\n",
    "        plt.ylabel('Amount')            \n",
    "        plt.title('Loan Amount by Age'); \n",
    "        \n",
    "        # calculating Amount distribution by Income\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.scatterplot(x='Income', y='Amount', data=data1)    \n",
    "        plt.xlabel('Income')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.title('Loan Amount by Income');\n",
    "        \n",
    "        #loan purpose count\n",
    "        plt.figure(figsize=(12,6))\n",
    "        data1[\"Intent\"].value_counts().plot(kind='bar')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Intent')\n",
    "        plt.title('Loan Intent Distribution');\n",
    "        \n",
    "        #check multicollinearity and correlation\n",
    "        plt.figure(figsize=(12,6))  \n",
    "        corr = data1.select_dtypes(include=['int64', 'float64']).drop('Amount', axis=1).corr()    \n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "        plt.title('Correlation Matrix');\n",
    "        \n",
    "        #drop column Id and Cred_length , Status, Default   \n",
    "        columns_to_drop = [\"Id\", \"Cred_length\", \"Status\", \"Default\"]\n",
    "        data1.drop(columns=[col for col in columns_to_drop if col in data1.columns], inplace=True)\n",
    "        pd.options.mode.copy_on_write=True\n",
    "        print(data1.head())\n",
    "        \n",
    "        data2 = data1\n",
    "        \n",
    "        return data2\n",
    "        \n",
    "    def feat_engineering(self):\n",
    "        data2 = pd.read_csv(self.config.data_path)\n",
    "        \n",
    "        #feature engineering\n",
    "        cat_features    = [\"Home\", \"Intent\"]\n",
    "        num_features   = [\"Age\",\t\"Income\", \"Emp_length\", \"Amount\",\"Rate\", \t\"Percent_income\"]\n",
    "        \n",
    "        #instantiate SimpleImputer\n",
    "       \n",
    "        \n",
    "        # instantiate the column StandardScaler\n",
    "        #numerical_processor = Pipeline(\n",
    "            #steps =[(\"standard scaling\",  StandardScaler()\n",
    "            #)]  \n",
    "        #)\n",
    "        \n",
    "        # instantiate the column OneHotEncoder\n",
    "        #categorical_processor = Pipeline(\n",
    "            #steps =[(\"one hot encoding\", OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "            #)]  \n",
    "        #)\n",
    "    \n",
    "        #implement the column transformer\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                \n",
    "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output= False), cat_features),\n",
    "                (\"num\", StandardScaler(), num_features)\n",
    "            ]\n",
    "        )   \n",
    "        \n",
    "        \n",
    "        pipeline= Pipeline(steps=[(preprocessor, \"preprocessor\")])\n",
    " \n",
    "        #fit the pipeline\n",
    "        pipeline.fit(data2)\n",
    "        \n",
    "        #save the pipeline\n",
    "        joblib.dump(pipeline, self.config.model_path)\n",
    "        \n",
    "        #transform the data\n",
    "        transformed_data = pipeline.transform(data2)\n",
    "        \n",
    "\n",
    "        data2=pd.DataFrame(transformed_data)\n",
    "        data2.columns = num_features + preprocessor.named_transformers_[\"cat\"].get_feature_names_out().tolist()\n",
    "        data2.to_csv(\"artifacts/data_ingestion/credit_risk.csv\", index=False)     \n",
    "        \n",
    "      \n",
    "\n",
    "        data3 = data2\n",
    "        \n",
    "        return data3 \n",
    "            \n",
    "        \n",
    "        \n",
    "    def train_test_splitting(self):\n",
    "        data3 = pd.read_csv(self.config.data_path)\n",
    "        \n",
    "        #split the data into train and test\n",
    "        train, test = train_test_split(data3, test_size=0.2, random_state=42)  \n",
    "        \n",
    "        train.to_csv(os.path.join(self.config.root_dir, 'train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, 'test.csv'), index=False)        #save the train and test data to the root directory     \n",
    "        \n",
    "        logger.info(\"Data split into train and test data\")  \n",
    "        logger.info(f\"Train data shape: {train.shape}\")         \n",
    "        logger.info(f\"Test data shape: {test.shape}\")  \n",
    "        \n",
    "        print(train.shape)\n",
    "        print(test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 21:20:52,965: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-17 21:20:52,972: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-17 21:20:52,976: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-17 21:20:52,978: INFO: common: created directory at: artifacts]\n",
      "[2025-03-17 21:20:52,979: INFO: common: created directory at: artifacts/data_transformation]\n",
      "...............................................................\n",
      "Age               0\n",
      "Income            0\n",
      "Home              0\n",
      "Emp_length        0\n",
      "Intent            0\n",
      "Amount            0\n",
      "Rate              0\n",
      "Percent_income    0\n",
      "Cred_length       0\n",
      "dtype: int64\n",
      "...............................................................\n",
      "[2025-03-17 21:20:53,039: INFO: 1468167966: Null values dropped]\n",
      "................................................................\n",
      "\n",
      "data.head()\n",
      "   Age  Income      Home  Emp_length     Intent  Amount   Rate  \\\n",
      "1   21    9600       OWN         5.0  EDUCATION    1000  11.14   \n",
      "2   25    9600  MORTGAGE         1.0    MEDICAL    5500  12.87   \n",
      "3   23   65500      RENT         4.0    MEDICAL   35000  15.23   \n",
      "4   24   54400      RENT         8.0    MEDICAL   35000  14.27   \n",
      "5   21    9900       OWN         2.0    VENTURE    2500   7.14   \n",
      "\n",
      "   Percent_income  Cred_length  \n",
      "1            0.10            2  \n",
      "2            0.57            3  \n",
      "3            0.53            2  \n",
      "4            0.55            4  \n",
      "5            0.25            2  \n",
      "...................................................................\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataTransformation.exploratory_data_analysis() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mtrain_test_splitting(transformed_data)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Perform data transformation steps\u001b[39;00m\n\u001b[0;32m      6\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mdata_cleaning()\n\u001b[1;32m----> 7\u001b[0m analyzed_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_transformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexploratory_data_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mfeat_engineering(analyzed_data)\n\u001b[0;32m      9\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mtrain_test_splitting(transformed_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: DataTransformation.exploratory_data_analysis() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config() \n",
    "    data_transformation = DataTransformation(config = data_transformation_config)\n",
    "    # Perform data transformation steps\n",
    "    cleaned_data = data_transformation.data_cleaning()\n",
    "    analyzed_data = data_transformation.exploratory_data_analysis(cleaned_data)\n",
    "    transformed_data = data_transformation.feat_engineering(analyzed_data)\n",
    "    train_data, test_data = data_transformation.train_test_splitting(transformed_data)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
